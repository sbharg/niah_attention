{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install transformers matplotlib pandas seaborn torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needles = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"retrieval_needles/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "df_needles = df_needles[df_needles[\"language\"] == \"English\"].reset_index(drop=True)\n",
    "\n",
    "df_haystack = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"en_haystack_texts/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "\n",
    "df_haystack = df_haystack[df_haystack[\"text\"].str.len() <= 2000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_needles(df_needles, n=5):\n",
    "    \"\"\"\n",
    "    Get random needles of different types from the dataframe.\n",
    "    \"\"\"\n",
    "    df = df_needles.sample(n=n)\n",
    "    df = df.drop_duplicates(subset=[\"arg1\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_context(df_haystack, df_needles) -> str:\n",
    "    \"\"\"\n",
    "    Generate context for the needles.\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for i in range(len(df_needles)):\n",
    "        needle = df_needles.iloc[i][\"needle\"]\n",
    "        haystack = \"\"\n",
    "        if i < len(df_haystack) - 1:\n",
    "            haystack = df_haystack.iloc[i][\"text\"]\n",
    "\n",
    "        context.append(needle)\n",
    "        if haystack:\n",
    "            context.append(\" \" + haystack + \" \")\n",
    "    return \"\".join(context)\n",
    "\n",
    "\n",
    "df_rand_needles = get_random_needles(df_needles, n=3)\n",
    "context = generate_context(df_haystack, df_rand_needles)\n",
    "\n",
    "prompt_needle = df_rand_needles.sample(n=1).reset_index(drop=True).iloc[0]\n",
    "\n",
    "sys_prompt = \"You are an intelligent AI assistant skilled in answering user questions base on documents provided by the user. Please keep your answers concise and clear. Do not talk about irrelevant topics or repeat your answers. The document given to you by the user is:\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": context,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_needle[\"retrieval_question\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

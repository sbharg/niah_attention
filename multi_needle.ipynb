{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install transformers matplotlib pandas seaborn torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5a385",
   "metadata": {},
   "source": [
    "#### utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04960fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_needles(df_needles, n=5):\n",
    "    \"\"\"\n",
    "    Get random needles of different types from the dataframe.\n",
    "    \"\"\"\n",
    "    df = df_needles.sample(n=n)\n",
    "    df = df.drop_duplicates(subset=[\"arg1\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_context(df_haystack, df_needles) -> str:\n",
    "    \"\"\"\n",
    "    Generate context for the promp.\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for i in range(len(df_needles)):\n",
    "        needle = df_needles.iloc[i][\"needle\"]\n",
    "        haystack = \"\"\n",
    "        if i < len(df_haystack) - 1:\n",
    "            haystack = df_haystack.iloc[i][\"text\"]\n",
    "\n",
    "        context.append(needle)\n",
    "        if haystack:\n",
    "            context.append(\" \" + haystack + \" \")\n",
    "    return \"\".join(context)\n",
    "\n",
    "\n",
    "def generate_messages(df_needles, df_haystack, n=5):\n",
    "    \"\"\"\n",
    "    Generate messages for the model.\n",
    "    Args:\n",
    "        df_needles (pd.DataFrame): DataFrame containing the needles.\n",
    "        df_haystack (pd.DataFrame): DataFrame containing the haystacks.\n",
    "        n (int): Number of random needles to select.\n",
    "    Returns:\n",
    "        messages (list): List of messages for the model.\n",
    "        prompt_needle (pd.Series): Random needle selected for the prompt.\n",
    "    \"\"\"\n",
    "    df_rand_needles = get_random_needles(df_needles, n=n)\n",
    "    context = generate_context(df_haystack, df_rand_needles)\n",
    "\n",
    "    prompt_needle = df_rand_needles.sample(n=1).reset_index(drop=True).iloc[0]\n",
    "\n",
    "    sys_prompt = \"You are an intelligent AI assistant skilled in answering user questions base on documents provided by the user. Please keep your answers concise and clear. Do not talk about irrelevant topics or repeat your answers. The document given to you by the user is:\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": context,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_needle[\"retrieval_question\"],\n",
    "        },\n",
    "    ]\n",
    "    return messages, prompt_needle\n",
    "\n",
    "\n",
    "def grade(response, answer) -> float:\n",
    "    return float(SequenceMatcher(None, response, answer).ratio())\n",
    "\n",
    "\n",
    "def find_subtensor_indices(\n",
    "    haystack: torch.LongTensor, needle: torch.LongTensor\n",
    ") -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns a 1D tensor of all start‐positions where `needle`\n",
    "    appears as a contiguous slice of `haystack`.\n",
    "    \"\"\"\n",
    "    n, m = haystack.size(0), needle.size(0)\n",
    "    if m == 0:\n",
    "        # every position (including “after” the last) is a match\n",
    "        return torch.arange(n + 1, dtype=torch.long)\n",
    "    if m > n:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # create all length‐m windows: shape (n-m+1, m)\n",
    "    windows = haystack.unfold(0, m, 1)  # → (n-m+1)×m\n",
    "    # compare each window to needle, then all dims must match\n",
    "    matches = (windows == needle).all(dim=1)  # → (n-m+1)\n",
    "    # extract the indices where True\n",
    "    return matches.nonzero(as_tuple=True)[0]\n",
    "\n",
    "\n",
    "def block_reduce(\n",
    "    matrix: torch.Tensor,\n",
    "    block_size: int = 64,\n",
    "    mode: str = \"max\",  # one of \"max\" or \"mean\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Collapse each contiguous block of `block_size` columns in every row of `matrix`\n",
    "    down to either its maximum or its average, returning a tensor of shape\n",
    "    (a, ceil(b/block_size)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : torch.Tensor\n",
    "        2D tensor of shape (a, b).\n",
    "    block_size : int\n",
    "        Number of columns per block (default 64).\n",
    "    mode : str\n",
    "        Reduction to apply: \"max\" or \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        2D tensor of shape (a, num_blocks) where num_blocks = ceil(b / block_size),\n",
    "        each entry is the max or mean over that block in the original row.\n",
    "    \"\"\"\n",
    "    if matrix.dim() != 2:\n",
    "        raise ValueError(\"`matrix` must be 2-dimensional\")\n",
    "    if mode not in {\"max\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'max' or 'mean'\")\n",
    "\n",
    "    a, b = matrix.shape\n",
    "    full_blocks = b // block_size\n",
    "\n",
    "    # handle all full blocks\n",
    "    if full_blocks > 0:\n",
    "        blocks = matrix[:, : full_blocks * block_size].unfold(\n",
    "            1, block_size, block_size\n",
    "        )  # → (a, full_blocks, block_size)\n",
    "        if mode == \"max\":\n",
    "            full_reduced, _ = blocks.max(dim=2)  # → (a, full_blocks)\n",
    "        else:  # mean\n",
    "            full_reduced = blocks.mean(dim=2)  # → (a, full_blocks)\n",
    "    else:\n",
    "        full_reduced = matrix.new_empty((a, 0))\n",
    "\n",
    "    # handle any remainder\n",
    "    rem = b - full_blocks * block_size\n",
    "    if rem > 0:\n",
    "        tail = matrix[:, full_blocks * block_size :]  # → (a, rem)\n",
    "        if mode == \"max\":\n",
    "            rem_reduced, _ = tail.max(dim=1, keepdim=True)  # → (a,1)\n",
    "        else:\n",
    "            rem_reduced = tail.mean(dim=1, keepdim=True)  # → (a,1)\n",
    "        return torch.cat([full_reduced, rem_reduced], dim=1)\n",
    "\n",
    "    return full_reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb040512",
   "metadata": {},
   "source": [
    "#### dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needles = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"retrieval_needles/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "df_needles = df_needles[df_needles[\"language\"] == \"English\"].reset_index(drop=True)\n",
    "\n",
    "df_haystack = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"en_haystack_texts/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "\n",
    "df_haystack = df_haystack[\n",
    "    df_haystack[\"text\"].str.len().between(5000, 7500)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "messages, prompt_needle = generate_messages(df_needles, df_haystack, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8817022",
   "metadata": {},
   "source": [
    "#### model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "if len(inputs.input_ids[0]) <= 6000:\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_LENGTH,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(inputs.input_ids, output.sequences)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "else:\n",
    "    print(f\"Text length: {len(text)}\")\n",
    "    print(f\"Context length ({len(inputs.input_ids[0])} tokens) too long\")\n",
    "    raise Exception(\n",
    "        f\"Text length: {len(text)}\\nContext length ({len(inputs.input_ids[0])} tokens) too long\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Context Length: {len(text)}\")\n",
    "print(prompt_needle[\"retrieval_question\"])\n",
    "print(prompt_needle[\"gold_standard_answer\"])\n",
    "print(response)\n",
    "print(grade(response, prompt_needle[\"gold_standard_answer\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52301d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m97 packages\u001b[0m \u001b[2min 0.37ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m78 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers datasets tiktoken matplotlib pandas seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7694db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4ec95",
   "metadata": {},
   "source": [
    "#### config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c69d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b294605",
   "metadata": {},
   "source": [
    "#### mrcr_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab4551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbharg/homework/ut_austin/cs391l_machine_learning/niah_attention/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def load_mrcr_parquet():\n",
    "    df = pd.read_parquet(\n",
    "        hf_hub_download(\n",
    "            repo_id=\"openai/mrcr\", filename=\"2needle.parquet\", repo_type=\"dataset\"\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def grade(response, answer, random_string_to_prepend) -> float:\n",
    "    if not response.startswith(random_string_to_prepend):\n",
    "        return 0\n",
    "    response = response.removeprefix(random_string_to_prepend)\n",
    "    answer = answer.removeprefix(random_string_to_prepend)\n",
    "    return float(SequenceMatcher(None, response, answer).ratio())\n",
    "\n",
    "\n",
    "def n_tokens(messages: list[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in messages.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return len(tokenizer(text).input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_mrcr_parquet()\n",
    "dataset = df[df[\"n_chars\"] < 20000]\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    messages = json.loads(row[\"prompt\"])\n",
    "    if len(row[\"prompt\"]) < 20000:\n",
    "        print(n_tokens(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.iloc[0]\n",
    "\n",
    "messages = json.loads(test[\"prompt\"])\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    output_attentions=True,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output.sequences)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "print(grade(response, test[\"answer\"], test[\"random_string_to_prepend\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1120e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subtensor_indices(\n",
    "    haystack: torch.LongTensor, needle: torch.LongTensor\n",
    ") -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns a 1D tensor of all start‐positions where `needle`\n",
    "    appears as a contiguous slice of `haystack`.\n",
    "    \"\"\"\n",
    "    n, m = haystack.size(0), needle.size(0)\n",
    "    if m == 0:\n",
    "        # every position (including “after” the last) is a match\n",
    "        return torch.arange(n + 1, dtype=torch.long)\n",
    "    if m > n:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # create all length‐m windows: shape (n-m+1, m)\n",
    "    windows = haystack.unfold(0, m, 1)  # → (n-m+1)×m\n",
    "    # compare each window to needle, then all dims must match\n",
    "    matches = (windows == needle).all(dim=1)  # → (n-m+1)\n",
    "    # extract the indices where True\n",
    "    return matches.nonzero(as_tuple=True)[0]\n",
    "\n",
    "\n",
    "random_string_tokens = tokenizer([\"<|im_start|>\"], return_tensors=\"pt\").to(model.device)\n",
    "random_string_tokens = random_string_tokens.input_ids[0]\n",
    "\n",
    "indices = find_subtensor_indices(inputs.input_ids[0], random_string_tokens)\n",
    "\n",
    "desired_msg_index = test[\"desired_msg_index\"]\n",
    "\n",
    "index_start = indices[desired_msg_index + 2].item() + 2\n",
    "index_end = indices[desired_msg_index + 3].item() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d690f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = output.attentions\n",
    "# attention_matrix_l1 = attention[0][0][0].cpu().float().numpy()\n",
    "\n",
    "# sns.heatmap(attention_matrix_l1[0], xticklabels=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\n",
    "#             yticklabels=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]), cmap=\"viridis\")\n",
    "# plt.title(\"Attention Weights\")\n",
    "# plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 7, figsize=(50, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    sns.heatmap(\n",
    "        attention[0][0][0][i][index_start:index_end].cpu().float().numpy(),\n",
    "        ax=ax,\n",
    "        cmap=\"bone\",\n",
    "        yticklabels=False,\n",
    "    )\n",
    "    ax.set_title(f\"Head {i + 1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52301d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m97 packages\u001b[0m \u001b[2min 0.37ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m78 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install auto_gptq\n",
    "!uv pip install optimum\n",
    "!uv pip install transformers matplotlib pandas seaborn torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4ec95",
   "metadata": {},
   "source": [
    "#### config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c69d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b294605",
   "metadata": {},
   "source": [
    "#### mrcr_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab4551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbharg/homework/ut_austin/cs391l_machine_learning/niah_attention/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def load_mrcr_parquet():\n",
    "    df = pd.read_parquet(\n",
    "        hf_hub_download(\n",
    "            repo_id=\"openai/mrcr\", filename=\"2needle.parquet\", repo_type=\"dataset\"\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def grade(response, answer, random_string_to_prepend) -> float:\n",
    "    # if not response.startswith(random_string_to_prepend):\n",
    "    #     return 0\n",
    "    response = response.removeprefix(random_string_to_prepend)\n",
    "    answer = answer.removeprefix(random_string_to_prepend)\n",
    "    return float(SequenceMatcher(None, response, answer).ratio())\n",
    "\n",
    "\n",
    "def n_tokens(messages: list[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in messages.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return len(tokenizer(text).input_ids)\n",
    "\n",
    "\n",
    "df = load_mrcr_parquet()\n",
    "dataset = df[df[\"n_chars\"] < 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.iloc[1]\n",
    "messages = json.loads(test[\"prompt\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "        output_attentions=True,\n",
    "        return_dict_in_generate=True,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output.sequences)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(response)\n",
    "print(grade(response, test[\"answer\"], test[\"random_string_to_prepend\"]))\n",
    "attention = output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1120e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subtensor_indices(\n",
    "    haystack: torch.LongTensor, needle: torch.LongTensor\n",
    ") -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns a 1D tensor of all start‐positions where `needle`\n",
    "    appears as a contiguous slice of `haystack`.\n",
    "    \"\"\"\n",
    "    n, m = haystack.size(0), needle.size(0)\n",
    "    if m == 0:\n",
    "        # every position (including “after” the last) is a match\n",
    "        return torch.arange(n + 1, dtype=torch.long)\n",
    "    if m > n:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # create all length‐m windows: shape (n-m+1, m)\n",
    "    windows = haystack.unfold(0, m, 1)  # → (n-m+1)×m\n",
    "    # compare each window to needle, then all dims must match\n",
    "    matches = (windows == needle).all(dim=1)  # → (n-m+1)\n",
    "    # extract the indices where True\n",
    "    return matches.nonzero(as_tuple=True)[0]\n",
    "\n",
    "\n",
    "random_string_tokens = tokenizer([\"<|im_start|>\"], return_tensors=\"pt\").to(model.device)\n",
    "random_string_tokens = random_string_tokens.input_ids[0]\n",
    "\n",
    "indices = find_subtensor_indices(inputs.input_ids[0], random_string_tokens)\n",
    "\n",
    "desired_msg_index = test[\"desired_msg_index\"]\n",
    "\n",
    "index_start = indices[desired_msg_index + 2].item() + 2\n",
    "index_end = indices[desired_msg_index + 3].item() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d690f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_reduce(\n",
    "    matrix: torch.Tensor,\n",
    "    block_size: int = 64,\n",
    "    mode: str = \"max\",  # one of \"max\" or \"mean\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Collapse each contiguous block of `block_size` columns in every row of `matrix`\n",
    "    down to either its maximum or its average, returning a tensor of shape\n",
    "    (a, ceil(b/block_size)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : torch.Tensor\n",
    "        2D tensor of shape (a, b).\n",
    "    block_size : int\n",
    "        Number of columns per block (default 64).\n",
    "    mode : str\n",
    "        Reduction to apply: \"max\" or \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        2D tensor of shape (a, num_blocks) where num_blocks = ceil(b / block_size),\n",
    "        each entry is the max or mean over that block in the original row.\n",
    "    \"\"\"\n",
    "    if matrix.dim() != 2:\n",
    "        raise ValueError(\"`matrix` must be 2-dimensional\")\n",
    "    if mode not in {\"max\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'max' or 'mean'\")\n",
    "\n",
    "    a, b = matrix.shape\n",
    "    full_blocks = b // block_size\n",
    "\n",
    "    # handle all full blocks\n",
    "    if full_blocks > 0:\n",
    "        blocks = matrix[:, : full_blocks * block_size].unfold(\n",
    "            1, block_size, block_size\n",
    "        )  # → (a, full_blocks, block_size)\n",
    "        if mode == \"max\":\n",
    "            full_reduced, _ = blocks.max(dim=2)  # → (a, full_blocks)\n",
    "        else:  # mean\n",
    "            full_reduced = blocks.mean(dim=2)  # → (a, full_blocks)\n",
    "    else:\n",
    "        full_reduced = matrix.new_empty((a, 0))\n",
    "\n",
    "    # handle any remainder\n",
    "    rem = b - full_blocks * block_size\n",
    "    if rem > 0:\n",
    "        tail = matrix[:, full_blocks * block_size :]  # → (a, rem)\n",
    "        if mode == \"max\":\n",
    "            rem_reduced, _ = tail.max(dim=1, keepdim=True)  # → (a,1)\n",
    "        else:\n",
    "            rem_reduced = tail.mean(dim=1, keepdim=True)  # → (a,1)\n",
    "        return torch.cat([full_reduced, rem_reduced], dim=1)\n",
    "\n",
    "    return full_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47de35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 28\n",
    "n_heads = 12\n",
    "div_factor = 4\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    fig, axes = plt.subplots(\n",
    "        div_factor, n_heads // div_factor, figsize=(8 * div_factor, 5 * div_factor)\n",
    "    )\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        weights = attention[0][layer][0][i][index_start:index_end]\n",
    "        weights = block_reduce(weights, mode=\"mean\").cpu().float().numpy()\n",
    "        sns.heatmap(\n",
    "            weights,\n",
    "            ax=ax,\n",
    "            cmap=\"bone\",\n",
    "            yticklabels=False,\n",
    "        )\n",
    "        ax.set_title(f\"Head {i + 1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"imgs_mean/layer_{layer + 1:02}_mean.png\")\n",
    "    plt.close()\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    fig, axes = plt.subplots(2, n_heads // 2, figsize=(50, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        weights = attention[0][layer][0][i][index_start:index_end]\n",
    "        weights = block_reduce(weights).cpu().float().numpy()\n",
    "        sns.heatmap(\n",
    "            weights,\n",
    "            ax=ax,\n",
    "            cmap=\"bone\",\n",
    "            yticklabels=False,\n",
    "        )\n",
    "        ax.set_title(f\"Head {i + 1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"imgs_max/layer_{layer + 1:02}_max.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

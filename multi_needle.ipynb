{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ad7432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers matplotlib pandas seaborn torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5a385",
   "metadata": {},
   "source": [
    "#### utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04960fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_needles(df_needles, n=5):\n",
    "    \"\"\"\n",
    "    Get random needles of different types from the dataframe.\n",
    "    \"\"\"\n",
    "    df = df_needles.sample(n=n)\n",
    "    df = df.drop_duplicates(subset=[\"arg1\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_context(df_haystack, df_needles) -> str:\n",
    "    \"\"\"\n",
    "    Generate context for the promp.\n",
    "    \"\"\"\n",
    "    context = [\" \"]\n",
    "    for i in range(len(df_needles)):\n",
    "        needle = df_needles.iloc[i][\"needle\"]\n",
    "        haystack = \"\"\n",
    "        if i < len(df_needles) - 1:\n",
    "            haystack = df_haystack.iloc[i][\"text\"]\n",
    "\n",
    "        context.append(needle)\n",
    "        if haystack:\n",
    "            context.append(\" \" + haystack + \" \")\n",
    "    return \"\".join(context)\n",
    "\n",
    "\n",
    "def generate_messages(df_needles, df_haystack, n=5):\n",
    "    \"\"\"\n",
    "    Generate messages for the model.\n",
    "    Args:\n",
    "        df_needles (pd.DataFrame): DataFrame containing the needles.\n",
    "        df_haystack (pd.DataFrame): DataFrame containing the haystacks.\n",
    "        n (int): Number of random needles to select.\n",
    "    Returns:\n",
    "        messages (list): List of messages for the model.\n",
    "        prompt_needle (pd.Series): Random needle selected for the prompt.\n",
    "    \"\"\"\n",
    "    df_rand_needles = get_random_needles(df_needles, n=n)\n",
    "    context = generate_context(df_haystack, df_rand_needles)\n",
    "\n",
    "    prompt_needle = df_rand_needles.sample(n=1).iloc[0]\n",
    "\n",
    "    sys_prompt = \"You are an intelligent AI assistant skilled in answering user questions base on documents provided by the user. Please keep your answers concise and clear. Do not talk about irrelevant topics or repeat your answers. The document given to you by the user is:\"\n",
    "    question, format = prompt_needle[\"retrieval_question\"].split(\"?\")\n",
    "    question += \"? Answer concisely, correctly, and in a complete sentence.\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": context,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        },\n",
    "    ]\n",
    "    return messages, prompt_needle, question\n",
    "\n",
    "\n",
    "def grade(response, answer) -> float:\n",
    "    return float(SequenceMatcher(None, response, answer).ratio())\n",
    "\n",
    "\n",
    "def find_subtensor_indices(\n",
    "    haystack: torch.LongTensor, needle: torch.LongTensor\n",
    ") -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns a 1D tensor of all start‐positions where `needle`\n",
    "    appears as a contiguous slice of `haystack`.\n",
    "    \"\"\"\n",
    "    n, m = haystack.size(0), needle.size(0)\n",
    "    if m == 0:\n",
    "        # every position (including “after” the last) is a match\n",
    "        return torch.arange(n + 1, dtype=torch.long)\n",
    "    if m > n:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # create all length‐m windows: shape (n-m+1, m)\n",
    "    windows = haystack.unfold(0, m, 1)  # → (n-m+1)×m\n",
    "    # compare each window to needle, then all dims must match\n",
    "    matches = (windows == needle).all(dim=1)  # → (n-m+1)\n",
    "    # extract the indices where True\n",
    "    return matches.nonzero(as_tuple=True)[0]\n",
    "\n",
    "\n",
    "def block_reduce(\n",
    "    matrix: torch.Tensor,\n",
    "    block_size: int = 64,\n",
    "    mode: str = \"max\",  # one of \"max\" or \"mean\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Collapse each contiguous block of `block_size` columns in every row of `matrix`\n",
    "    down to either its maximum or its average, returning a tensor of shape\n",
    "    (a, ceil(b/block_size)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : torch.Tensor\n",
    "        2D tensor of shape (a, b).\n",
    "    block_size : int\n",
    "        Number of columns per block (default 64).\n",
    "    mode : str\n",
    "        Reduction to apply: \"max\" or \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        2D tensor of shape (a, num_blocks) where num_blocks = ceil(b / block_size),\n",
    "        each entry is the max or mean over that block in the original row.\n",
    "    \"\"\"\n",
    "    if matrix.dim() != 2:\n",
    "        raise ValueError(\"`matrix` must be 2-dimensional\")\n",
    "    if mode not in {\"max\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'max' or 'mean'\")\n",
    "\n",
    "    a, b = matrix.shape\n",
    "    full_blocks = b // block_size\n",
    "\n",
    "    # handle all full blocks\n",
    "    if full_blocks > 0:\n",
    "        blocks = matrix[:, : full_blocks * block_size].unfold(\n",
    "            1, block_size, block_size\n",
    "        )  # → (a, full_blocks, block_size)\n",
    "        if mode == \"max\":\n",
    "            full_reduced, _ = blocks.max(dim=2)  # → (a, full_blocks)\n",
    "        else:  # mean\n",
    "            full_reduced = blocks.mean(dim=2)  # → (a, full_blocks)\n",
    "    else:\n",
    "        full_reduced = matrix.new_empty((a, 0))\n",
    "\n",
    "    # handle any remainder\n",
    "    rem = b - full_blocks * block_size\n",
    "    if rem > 0:\n",
    "        tail = matrix[:, full_blocks * block_size :]  # → (a, rem)\n",
    "        if mode == \"max\":\n",
    "            rem_reduced, _ = tail.max(dim=1, keepdim=True)  # → (a,1)\n",
    "        else:\n",
    "            rem_reduced = tail.mean(dim=1, keepdim=True)  # → (a,1)\n",
    "        return torch.cat([full_reduced, rem_reduced], dim=1)\n",
    "\n",
    "    return full_reduced\n",
    "\n",
    "\n",
    "def aggregate_tensors(\n",
    "    tensors: List[torch.Tensor], mode: str = \"mean\", percentile: Optional[float] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Aggregate a list of (a,b) tensors entrywise.\n",
    "\n",
    "    Args:\n",
    "      tensors: list of torch.Tensor of identical shape.\n",
    "      mode: one of {\"mean\", \"median\", \"percentile\"}.\n",
    "      percentile: if mode==\"percentile\", the desired percentile in [0,100].\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape (a,b) where each entry is the requested\n",
    "      statistic over that position across all input tensors.\n",
    "\n",
    "    Raises:\n",
    "      ValueError if inputs are invalid or percentile is out of range.\n",
    "    \"\"\"\n",
    "    if not tensors:\n",
    "        raise ValueError(\"Need at least one tensor\")\n",
    "    shape = tensors[0].shape\n",
    "    for t in tensors:\n",
    "        if t.shape != shape:\n",
    "            raise ValueError(\"All tensors must have the same shape\")\n",
    "\n",
    "    # stack into (N,a,b) and convert to float\n",
    "    stacked = torch.stack(tensors, dim=0).float()\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        return stacked.mean(dim=0)\n",
    "\n",
    "    elif mode == \"median\":\n",
    "        vals, _ = stacked.median(dim=0)\n",
    "        return vals\n",
    "\n",
    "    elif mode == \"percentile\":\n",
    "        if percentile is None:\n",
    "            raise ValueError(\"Must specify percentile when mode='percentile'\")\n",
    "        if not (0 <= percentile <= 100):\n",
    "            raise ValueError(\"percentile must be between 0 and 100\")\n",
    "        # torch.quantile takes q in [0.,1.]\n",
    "        q = percentile / 100.0\n",
    "        return torch.quantile(stacked, q, dim=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode '{mode}'. Choose mean, median, or percentile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb040512",
   "metadata": {},
   "source": [
    "#### dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df_needles = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"retrieval_needles/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "df_needles = df_needles[df_needles[\"language\"] == \"English\"].reset_index(drop=True)\n",
    "\n",
    "df_haystack = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"en_haystack_texts/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "\n",
    "df_haystack = df_haystack[\n",
    "    df_haystack[\"text\"].str.len().between(5000, 7500)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8817022",
   "metadata": {},
   "source": [
    "#### model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_needles = 6\n",
    "messages, prompt_needle, question = generate_messages(\n",
    "    df_needles, df_haystack, n=n_needles\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "if len(inputs.input_ids[0]) <= 1250 * n_needles:\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_LENGTH,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(inputs.input_ids, output.sequences)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "else:\n",
    "    print(f\"Text length: {len(text)}\")\n",
    "    print(f\"Context length ({len(inputs.input_ids[0])} tokens) too long\")\n",
    "    raise Exception(\n",
    "        f\"Text length: {len(text)}\\nContext length ({len(inputs.input_ids[0])} tokens) too long\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text Length: {len(text)}\")\n",
    "print(f\"Context Length: {len(inputs.input_ids[0])}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Correct Answer: {prompt_needle['gold_standard_answer']}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Grade: {grade(response, prompt_needle['gold_standard_answer'])}\")\n",
    "print(f\"Needle Position: {prompt_needle.name}\")\n",
    "\n",
    "with open(\"log.txt\", \"w\") as f:\n",
    "    f.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "    f.write(f\"Text Length: {len(text)}\\n\")\n",
    "    f.write(f\"Context Length: {len(inputs.input_ids[0])}\\n\")\n",
    "    f.write(f\"Question: {question}\\n\")\n",
    "    f.write(f\"Correct Answer: {prompt_needle['gold_standard_answer']}\\n\")\n",
    "    f.write(f\"Response: {response}\\n\")\n",
    "    f.write(f\"Grade: {grade(response, prompt_needle['gold_standard_answer'])}\\n\")\n",
    "    f.write(f\"Needle Position (zero indexed): {prompt_needle.name}\\n\")\n",
    "    f.write(f\"Total Number of Needles: {n_needles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_string_tokens = tokenizer(\n",
    "    [\" \" + prompt_needle[\"needle\"]], return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "random_string_tokens = random_string_tokens.input_ids[0]\n",
    "\n",
    "indices = find_subtensor_indices(inputs.input_ids[0], random_string_tokens)\n",
    "\n",
    "index_start = indices[0].item()\n",
    "index_end = index_start + len(random_string_tokens)\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_heads = model.config.num_attention_heads\n",
    "div_factor = 2\n",
    "\n",
    "Path(\"imgs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "attention = output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_string_tokens = tokenizer([\"<|im_start|>\"], return_tensors=\"pt\").to(model.device)\n",
    "random_string_tokens = random_string_tokens.input_ids[0]\n",
    "\n",
    "msg_indices = find_subtensor_indices(inputs.input_ids[0], random_string_tokens)\n",
    "\n",
    "# desired_msg_index = test[\"desired_msg_index\"]\n",
    "\n",
    "# index_start = indices[desired_msg_index + 2].item() + 2\n",
    "# index_end = indices[desired_msg_index + 3].item() - 2\n",
    "\n",
    "print(msg_indices)\n",
    "print(\n",
    "    \"\".join(\n",
    "        tokenizer.batch_decode(\n",
    "            inputs.input_ids[0][msg_indices[-2] + 2 : msg_indices[-1] - 2]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58377beb",
   "metadata": {},
   "source": [
    "#### heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps(head_weights, agg_mode=\"mean\", save_title=\"imgs/avg_per_head.png\"):\n",
    "    fig, axes = plt.subplots(\n",
    "        div_factor, n_heads // div_factor, figsize=(15 * div_factor, 5 * div_factor)\n",
    "    )\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        weights = (\n",
    "            aggregate_tensors(head_weights[i], mode=agg_mode, percentile=99)\n",
    "            .cpu()\n",
    "            .float()\n",
    "            .numpy()\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            weights,\n",
    "            ax=ax,\n",
    "            cmap=\"rocket\",\n",
    "            yticklabels=False,\n",
    "        )\n",
    "        ax.set_title(f\"Head {i + 1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_title)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63efeee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [msg_indices[-2] + 2, msg_indices[-1] - 2]\n",
    "layer_range = [1, 12]\n",
    "block_size = 32\n",
    "\n",
    "head_weights = [\n",
    "    [\n",
    "        block_reduce(\n",
    "            attention[0][layer][0][i][idx[0] : idx[1]],\n",
    "            mode=\"mean\",\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        # block_reduce(\n",
    "        #     torch.cat(\n",
    "        #         (\n",
    "        #             attention[0][layer][0][i][index_start:index_end],\n",
    "        #             attention[0][layer][0][i][3401 + 2 : 3868 - 2],\n",
    "        #         ),\n",
    "        #         dim=0,\n",
    "        #     ),\n",
    "        #     mode=\"mean\",\n",
    "        # )\n",
    "        for layer in range(layer_range[0] - 1, layer_range[1])\n",
    "    ]\n",
    "    for i in range(n_heads)\n",
    "]\n",
    "\n",
    "plot_heatmaps(\n",
    "    head_weights, agg_mode=\"mean\", save_title=\"imgs/avg_per_head_mer_layers112.png\"\n",
    ")\n",
    "plot_heatmaps(\n",
    "    head_weights,\n",
    "    agg_mode=\"percentile\",\n",
    "    save_title=\"imgs/99_percentile_per_head_mer_layers112.png\",\n",
    ")\n",
    "\n",
    "head_weights = [\n",
    "    [\n",
    "        block_reduce(\n",
    "            attention[0][layer][0][i][idx[0] : idx[1]],\n",
    "            mode=\"max\",\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        # block_reduce(\n",
    "        #     torch.cat(\n",
    "        #         (\n",
    "        #             attention[0][layer][0][i][index_start:index_end],\n",
    "        #             attention[0][layer][0][i][3401 + 2 : 3868 - 2],\n",
    "        #         ),\n",
    "        #         dim=0,\n",
    "        #     ),\n",
    "        #     mode=\"mean\",\n",
    "        # )\n",
    "        for layer in range(layer_range[0] - 1, layer_range[1])\n",
    "    ]\n",
    "    for i in range(n_heads)\n",
    "]\n",
    "\n",
    "plot_heatmaps(\n",
    "    head_weights, agg_mode=\"mean\", save_title=\"imgs/avg_per_head_mxr_layers112.png\"\n",
    ")\n",
    "plot_heatmaps(\n",
    "    head_weights,\n",
    "    agg_mode=\"percentile\",\n",
    "    save_title=\"imgs/99_percentile_per_head_mxr_layers112.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [msg_indices[-2] + 2, msg_indices[-1] - 2]\n",
    "layer_range = [13, 24]\n",
    "block_size = 32\n",
    "\n",
    "head_weights = [\n",
    "    [\n",
    "        block_reduce(\n",
    "            attention[0][layer][0][i][idx[0] : idx[1]],\n",
    "            mode=\"mean\",\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        # block_reduce(\n",
    "        #     torch.cat(\n",
    "        #         (\n",
    "        #             attention[0][layer][0][i][index_start:index_end],\n",
    "        #             attention[0][layer][0][i][3401 + 2 : 3868 - 2],\n",
    "        #         ),\n",
    "        #         dim=0,\n",
    "        #     ),\n",
    "        #     mode=\"mean\",\n",
    "        # )\n",
    "        for layer in range(layer_range[0] - 1, layer_range[1])\n",
    "    ]\n",
    "    for i in range(n_heads)\n",
    "]\n",
    "\n",
    "plot_heatmaps(\n",
    "    head_weights, agg_mode=\"mean\", save_title=\"imgs/avg_per_head_mer_layers1324.png\"\n",
    ")\n",
    "plot_heatmaps(\n",
    "    head_weights,\n",
    "    agg_mode=\"percentile\",\n",
    "    save_title=\"imgs/99_percentile_per_head_mer_layers1324.png\",\n",
    ")\n",
    "\n",
    "head_weights = [\n",
    "    [\n",
    "        block_reduce(\n",
    "            attention[0][layer][0][i][idx[0] : idx[1]],\n",
    "            mode=\"max\",\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        # block_reduce(\n",
    "        #     torch.cat(\n",
    "        #         (\n",
    "        #             attention[0][layer][0][i][index_start:index_end],\n",
    "        #             attention[0][layer][0][i][3401 + 2 : 3868 - 2],\n",
    "        #         ),\n",
    "        #         dim=0,\n",
    "        #     ),\n",
    "        #     mode=\"mean\",\n",
    "        # )\n",
    "        for layer in range(layer_range[0] - 1, layer_range[1])\n",
    "    ]\n",
    "    for i in range(n_heads)\n",
    "]\n",
    "\n",
    "plot_heatmaps(\n",
    "    head_weights, agg_mode=\"mean\", save_title=\"imgs/avg_per_head_mxr_layers1324.png\"\n",
    ")\n",
    "plot_heatmaps(\n",
    "    head_weights,\n",
    "    agg_mode=\"percentile\",\n",
    "    save_title=\"imgs/99_percentile_per_head_mxr_layers1324.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r /content/imgs.zip /content/imgs_max/ /content/imgs_mean/ /content/log.txt\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download(\"/content/imgs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353188c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in range(n_layers):\n",
    "#     fig, axes = plt.subplots(\n",
    "#         div_factor, n_heads // div_factor, figsize=(25 * div_factor, 5 * div_factor)\n",
    "#     )\n",
    "#     for i, ax in enumerate(axes.flat):\n",
    "#         weights = attention[0][layer][0][i][index_start:index_end]\n",
    "#         weights = block_reduce(weights, mode=\"mean\").cpu().float().numpy()\n",
    "#         sns.heatmap(\n",
    "#             weights,\n",
    "#             ax=ax,\n",
    "#             cmap=\"bone\",\n",
    "#             yticklabels=False,\n",
    "#         )\n",
    "#         ax.set_title(f\"Head {i + 1}\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"imgs_mean/layer_{layer + 1:02}_mean.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# for layer in range(n_layers):\n",
    "#     fig, axes = plt.subplots(\n",
    "#         div_factor, n_heads // div_factor, figsize=(25 * div_factor, 5 * div_factor)\n",
    "#     )\n",
    "#     for i, ax in enumerate(axes.flat):\n",
    "#         weights = attention[0][layer][0][i][index_start:index_end]\n",
    "#         weights = block_reduce(weights).cpu().float().numpy()\n",
    "#         sns.heatmap(\n",
    "#             weights,\n",
    "#             ax=ax,\n",
    "#             cmap=\"bone\",\n",
    "#             yticklabels=False,\n",
    "#         )\n",
    "#         ax.set_title(f\"Head {i + 1}\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"imgs_max/layer_{layer + 1:02}_max.png\")\n",
    "#     plt.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

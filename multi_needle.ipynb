{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install transformers matplotlib pandas seaborn torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5a385",
   "metadata": {},
   "source": [
    "#### utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04960fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_needles(df_needles, n=5):\n",
    "    \"\"\"\n",
    "    Get random needles of different types from the dataframe.\n",
    "    \"\"\"\n",
    "    df = df_needles.sample(n=n)\n",
    "    df = df.drop_duplicates(subset=[\"arg1\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_context(df_haystack, df_needles) -> str:\n",
    "    \"\"\"\n",
    "    Generate context for the promp.\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for i in range(len(df_needles)):\n",
    "        needle = df_needles.iloc[i][\"needle\"]\n",
    "        haystack = \"\"\n",
    "        if i < len(df_haystack) - 1:\n",
    "            haystack = df_haystack.iloc[i][\"text\"]\n",
    "\n",
    "        context.append(needle)\n",
    "        if haystack:\n",
    "            context.append(\" \" + haystack + \" \")\n",
    "    return \"\".join(context)\n",
    "\n",
    "\n",
    "def generate_messages(df_needles, df_haystack, n=5):\n",
    "    \"\"\"\n",
    "    Generate messages for the model.\n",
    "    Args:\n",
    "        df_needles (pd.DataFrame): DataFrame containing the needles.\n",
    "        df_haystack (pd.DataFrame): DataFrame containing the haystacks.\n",
    "        n (int): Number of random needles to select.\n",
    "    Returns:\n",
    "        messages (list): List of messages for the model.\n",
    "        prompt_needle (pd.Series): Random needle selected for the prompt.\n",
    "    \"\"\"\n",
    "    df_rand_needles = get_random_needles(df_needles, n=n)\n",
    "    context = generate_context(df_haystack, df_rand_needles)\n",
    "\n",
    "    prompt_needle = df_rand_needles.sample(n=1).reset_index(drop=True).iloc[0]\n",
    "\n",
    "    sys_prompt = \"You are an intelligent AI assistant skilled in answering user questions base on documents provided by the user. Please keep your answers concise and clear. Do not talk about irrelevant topics or repeat your answers. The document given to you by the user is:\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": context,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_needle[\"retrieval_question\"],\n",
    "        },\n",
    "    ]\n",
    "    return messages, prompt_needle\n",
    "\n",
    "\n",
    "def grade(response, answer) -> float:\n",
    "    return float(SequenceMatcher(None, response, answer).ratio())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb040512",
   "metadata": {},
   "source": [
    "#### dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needles = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"retrieval_needles/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "df_needles = df_needles[df_needles[\"language\"] == \"English\"].reset_index(drop=True)\n",
    "\n",
    "df_haystack = pd.read_parquet(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"opencompass/NeedleBench\",\n",
    "        filename=\"en_haystack_texts/test/0000.parquet\",\n",
    "        repo_type=\"dataset\",\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    ")\n",
    "\n",
    "df_haystack = df_haystack[\n",
    "    df_haystack[\"text\"].str.len().between(5000, 7500)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "messages, prompt_needle = generate_messages(df_needles, df_haystack, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8817022",
   "metadata": {},
   "source": [
    "#### model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "if len(inputs.input_ids[0]) <= 6000:\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_LENGTH,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(inputs.input_ids, output.sequences)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "else:\n",
    "    print(f\"Text length: {len(text)}\")\n",
    "    print(f\"Context length ({len(inputs.input_ids[0])} tokens) too long\")\n",
    "    raise Exception(\n",
    "        f\"Text length: {len(text)}\\nContext length ({len(inputs.input_ids[0])} tokens) too long\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Context Length: {len(text)}\")\n",
    "print(prompt_needle[\"retrieval_question\"])\n",
    "print(prompt_needle[\"gold_standard_answer\"])\n",
    "print(response)\n",
    "print(grade(response, prompt_needle[\"gold_standard_answer\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

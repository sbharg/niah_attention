{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52301d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers matplotlib pandas seaborn torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4ec95",
   "metadata": {},
   "source": [
    "#### model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c69d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b294605",
   "metadata": {},
   "source": [
    "#### utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab4551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbharg/homework/ut_austin/cs391l_machine_learning/niah_attention/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def load_mrcr_parquet():\n",
    "    df = pd.read_parquet(\n",
    "        hf_hub_download(\n",
    "            repo_id=\"openai/mrcr\", filename=\"2needle.parquet\", repo_type=\"dataset\"\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def grade(response, answer, random_string_to_prepend) -> float:\n",
    "    # if not response.startswith(random_string_to_prepend):\n",
    "    #     return 0\n",
    "    response = response.removeprefix(random_string_to_prepend)\n",
    "    answer = answer.removeprefix(random_string_to_prepend)\n",
    "    return float(SequenceMatcher(None, response, answer).ratio())\n",
    "\n",
    "\n",
    "def n_tokens(messages: List[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in messages.\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return len(tokenizer(text).input_ids)\n",
    "\n",
    "\n",
    "def find_subtensor_indices(\n",
    "    haystack: torch.LongTensor, needle: torch.LongTensor\n",
    ") -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns a 1D tensor of all start‐positions where `needle`\n",
    "    appears as a contiguous slice of `haystack`.\n",
    "    \"\"\"\n",
    "    n, m = haystack.size(0), needle.size(0)\n",
    "    if m == 0:\n",
    "        # every position (including “after” the last) is a match\n",
    "        return torch.arange(n + 1, dtype=torch.long)\n",
    "    if m > n:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # create all length‐m windows: shape (n-m+1, m)\n",
    "    windows = haystack.unfold(0, m, 1)  # → (n-m+1)×m\n",
    "    # compare each window to needle, then all dims must match\n",
    "    matches = (windows == needle).all(dim=1)  # → (n-m+1)\n",
    "    # extract the indices where True\n",
    "    return matches.nonzero(as_tuple=True)[0]\n",
    "\n",
    "\n",
    "def block_reduce(\n",
    "    matrix: torch.Tensor,\n",
    "    block_size: int = 64,\n",
    "    mode: str = \"max\",  # one of \"max\" or \"mean\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Collapse each contiguous block of `block_size` columns in every row of `matrix`\n",
    "    down to either its maximum or its average, returning a tensor of shape\n",
    "    (a, ceil(b/block_size)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : torch.Tensor\n",
    "        2D tensor of shape (a, b).\n",
    "    block_size : int\n",
    "        Number of columns per block (default 64).\n",
    "    mode : str\n",
    "        Reduction to apply: \"max\" or \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        2D tensor of shape (a, num_blocks) where num_blocks = ceil(b / block_size),\n",
    "        each entry is the max or mean over that block in the original row.\n",
    "    \"\"\"\n",
    "    if matrix.dim() != 2:\n",
    "        raise ValueError(\"`matrix` must be 2-dimensional\")\n",
    "    if mode not in {\"max\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'max' or 'mean'\")\n",
    "\n",
    "    a, b = matrix.shape\n",
    "    full_blocks = b // block_size\n",
    "\n",
    "    # handle all full blocks\n",
    "    if full_blocks > 0:\n",
    "        blocks = matrix[:, : full_blocks * block_size].unfold(\n",
    "            1, block_size, block_size\n",
    "        )  # → (a, full_blocks, block_size)\n",
    "        if mode == \"max\":\n",
    "            full_reduced, _ = blocks.max(dim=2)  # → (a, full_blocks)\n",
    "        else:  # mean\n",
    "            full_reduced = blocks.mean(dim=2)  # → (a, full_blocks)\n",
    "    else:\n",
    "        full_reduced = matrix.new_empty((a, 0))\n",
    "\n",
    "    # handle any remainder\n",
    "    rem = b - full_blocks * block_size\n",
    "    if rem > 0:\n",
    "        tail = matrix[:, full_blocks * block_size :]  # → (a, rem)\n",
    "        if mode == \"max\":\n",
    "            rem_reduced, _ = tail.max(dim=1, keepdim=True)  # → (a,1)\n",
    "        else:\n",
    "            rem_reduced = tail.mean(dim=1, keepdim=True)  # → (a,1)\n",
    "        return torch.cat([full_reduced, rem_reduced], dim=1)\n",
    "\n",
    "    return full_reduced\n",
    "\n",
    "\n",
    "def aggregate_tensors(\n",
    "    tensors: List[torch.Tensor], mode: str = \"mean\", percentile: Optional[float] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Aggregate a list of (a,b) tensors entrywise.\n",
    "\n",
    "    Args:\n",
    "      tensors: list of torch.Tensor of identical shape.\n",
    "      mode: one of {\"mean\", \"median\", \"percentile\"}.\n",
    "      percentile: if mode==\"percentile\", the desired percentile in [0,100].\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape (a,b) where each entry is the requested\n",
    "      statistic over that position across all input tensors.\n",
    "\n",
    "    Raises:\n",
    "      ValueError if inputs are invalid or percentile is out of range.\n",
    "    \"\"\"\n",
    "    if not tensors:\n",
    "        raise ValueError(\"Need at least one tensor\")\n",
    "    shape = tensors[0].shape\n",
    "    for t in tensors:\n",
    "        if t.shape != shape:\n",
    "            raise ValueError(\"All tensors must have the same shape\")\n",
    "\n",
    "    # stack into (N,a,b) and convert to float\n",
    "    stacked = torch.stack(tensors, dim=0).float()\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        return stacked.mean(dim=0)\n",
    "\n",
    "    elif mode == \"median\":\n",
    "        vals, _ = stacked.median(dim=0)\n",
    "        return vals\n",
    "\n",
    "    elif mode == \"percentile\":\n",
    "        if percentile is None:\n",
    "            raise ValueError(\"Must specify percentile when mode='percentile'\")\n",
    "        if not (0 <= percentile <= 100):\n",
    "            raise ValueError(\"percentile must be between 0 and 100\")\n",
    "        # torch.quantile takes q in [0.,1.]\n",
    "        q = percentile / 100.0\n",
    "        return torch.quantile(stacked, q, dim=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode '{mode}'. Choose mean, median, or percentile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_mrcr_parquet()\n",
    "dataset = df[df[\"n_chars\"] < 20000]\n",
    "\n",
    "test = dataset.iloc[1]\n",
    "messages = json.loads(test[\"prompt\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "        output_attentions=True,\n",
    "        return_dict_in_generate=True,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output.sequences)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(response)\n",
    "score = grade(response, test[\"answer\"], test[\"random_string_to_prepend\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text Length: {len(text)}\")\n",
    "print(f\"Context Length: {len(inputs.input_ids[0])}\")\n",
    "print(f\"Question: {test['prompt'][-1]}\")\n",
    "print(f\"Correct Answer: {test['answer']}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Grade: {score}\")\n",
    "print(f\"Needle Position: {test['desired_msg_index']}\")\n",
    "\n",
    "with open(\"log.txt\", \"w\") as f:\n",
    "    f.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "    f.write(f\"Text Length: {len(text)}\\n\")\n",
    "    f.write(f\"Context Length: {len(inputs.input_ids[0])}\\n\")\n",
    "    f.write(f\"Question: {test['prompt'][-1]}\\n\")\n",
    "    f.write(f\"Correct Answer: {test['answer']}\\n\")\n",
    "    f.write(f\"Response: {response}\\n\")\n",
    "    f.write(f\"Grade: {score}\\n\")\n",
    "    f.write(f\"Needle Position (one indexed): {test['desired_msg_index']}\\n\")\n",
    "    f.write(f\"Total Number of Messages: {test['total_messages']}\\n\")\n",
    "    f.write(f\"Total Number of Needles: {test['n_needles']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1120e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_string_tokens = tokenizer([\"<|im_start|>\"], return_tensors=\"pt\").to(model.device)\n",
    "random_string_tokens = random_string_tokens.input_ids[0]\n",
    "\n",
    "indices = find_subtensor_indices(inputs.input_ids[0], random_string_tokens)\n",
    "\n",
    "desired_msg_index = test[\"desired_msg_index\"]\n",
    "\n",
    "index_start = indices[desired_msg_index + 2].item() + 2\n",
    "index_end = indices[desired_msg_index + 3].item() - 2\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_heads = model.config.num_attention_heads\n",
    "div_factor = 4\n",
    "\n",
    "Path(\"imgs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "attention = output.attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafeaa96",
   "metadata": {},
   "source": [
    "#### heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "head_weights = [\n",
    "    [\n",
    "        block_reduce(attention[0][layer][0][i], mode=\"max\")\n",
    "        # block_reduce(\n",
    "        #     torch.cat(\n",
    "        #         (\n",
    "        #             attention[0][layer][0][i][index_start:index_end],\n",
    "        #             attention[0][layer][0][i][3401 + 2 : 3868 - 2],\n",
    "        #         ),\n",
    "        #         dim=0,\n",
    "        #     ),\n",
    "        #     mode=\"mean\",\n",
    "        # )\n",
    "        for layer in range(n_layers)\n",
    "    ]\n",
    "    for i in range(n_heads)\n",
    "]\n",
    "\n",
    "\n",
    "def add_patches(ax, ranges, colors):\n",
    "    for i, (x0, x1) in enumerate(ranges):\n",
    "        col = colors[i] if colors and i < len(colors) else \"red\"\n",
    "        x, y, w, h = -0.1, x0, -1, x1 - x0 + 1\n",
    "        ax.add_patch(\n",
    "            Rectangle((x, y), w, h, fill=True, facecolor=col, lw=4, clip_on=False)\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_heatmaps(ranges, colors, agg_mode=\"mean\", save_title=\"imgs/avg_per_head.png\"):\n",
    "    fig, axes = plt.subplots(\n",
    "        div_factor, n_heads // div_factor, figsize=(8 * div_factor, 5 * div_factor)\n",
    "    )\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        weights = (\n",
    "            aggregate_tensors(head_weights[i], mode=agg_mode, percentile=99)\n",
    "            .cpu()\n",
    "            .float()\n",
    "            .numpy()\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            weights,\n",
    "            ax=ax,\n",
    "            cmap=\"rocket\",\n",
    "            yticklabels=False,\n",
    "        )\n",
    "        ax.set_title(f\"Head {i + 1}\")\n",
    "        add_patches(ax, ranges, colors)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_title)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "ranges = [(index_start, index_end), (3401 + 2, 3868 - 2)]\n",
    "colors = [\"orange\", \"green\"]\n",
    "plot_heatmaps(ranges, colors, agg_mode=\"mean\", save_title=\"imgs/avg_per_head.png\")\n",
    "plot_heatmaps(\n",
    "    ranges, colors, agg_mode=\"percentile\", save_title=\"imgs/99_percentile_per_head.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r /content/imgs.zip /content/imgs/ /content/log.txt\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download(\"/content/imgs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in range(n_layers):\n",
    "#     fig, axes = plt.subplots(\n",
    "#         div_factor, n_heads // div_factor, figsize=(8 * div_factor, 5 * div_factor)\n",
    "#     )\n",
    "#     for i, ax in enumerate(axes.flat):\n",
    "#         weights = attention[0][layer][0][i][index_start:index_end]\n",
    "#         weights = block_reduce(weights, mode=\"mean\").cpu().float().numpy()\n",
    "#         sns.heatmap(\n",
    "#             weights,\n",
    "#             ax=ax,\n",
    "#             cmap=\"bone\",\n",
    "#             yticklabels=False,\n",
    "#         )\n",
    "#         ax.set_title(f\"Head {i + 1}\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"imgs_mean/layer_{layer + 1:02}_mean.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# for layer in range(n_layers):\n",
    "#     fig, axes = plt.subplots(2, n_heads // 2, figsize=(50, 10))\n",
    "#     for i, ax in enumerate(axes.flat):\n",
    "#         weights = attention[0][layer][0][i][index_start:index_end]\n",
    "#         weights = block_reduce(weights).cpu().float().numpy()\n",
    "#         sns.heatmap(\n",
    "#             weights,\n",
    "#             ax=ax,\n",
    "#             cmap=\"bone\",\n",
    "#             yticklabels=False,\n",
    "#         )\n",
    "#         ax.set_title(f\"Head {i + 1}\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"imgs_max/layer_{layer + 1:02}_max.png\")\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7737fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_weights = [\n",
    "#     [\n",
    "#         block_reduce(\n",
    "#             torch.cat(\n",
    "#                 (\n",
    "#                     attention[0][layer][0][i][index_start:index_end],\n",
    "#                     attention[0][layer][0][i][3401 + 2 : 3868 - 2],\n",
    "#                 ),\n",
    "#                 dim=0,\n",
    "#             ),\n",
    "#             mode=\"mean\",\n",
    "#         )\n",
    "#         for layer in range(n_layers)\n",
    "#     ]\n",
    "#     for i in range(n_heads)\n",
    "# ]\n",
    "\n",
    "# fig, axes = plt.subplots(\n",
    "#     div_factor, n_heads // div_factor, figsize=(10 * div_factor, 5 * div_factor)\n",
    "# )\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     weights = aggregate_tensors(head_weights[i], mode=\"mean\").cpu().float().numpy()\n",
    "#     sns.heatmap(\n",
    "#         weights,\n",
    "#         ax=ax,\n",
    "#         cmap=\"rocket\",\n",
    "#         yticklabels=False,\n",
    "#     )\n",
    "#     ax.set_title(f\"Head {i + 1}\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"imgs/avg_per_head.png\")\n",
    "# plt.close()\n",
    "\n",
    "# fig, axes = plt.subplots(\n",
    "#     div_factor, n_heads // div_factor, figsize=(10 * div_factor, 5 * div_factor)\n",
    "# )\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     weights = aggregate_tensors(head_weights[i], mode=\"median\").cpu().float().numpy()\n",
    "#     sns.heatmap(\n",
    "#         weights,\n",
    "#         ax=ax,\n",
    "#         cmap=\"rocket\",\n",
    "#         yticklabels=False,\n",
    "#     )\n",
    "#     ax.set_title(f\"Head {i + 1}\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"imgs/median_per_head.png\")\n",
    "# plt.close()\n",
    "\n",
    "# fig, axes = plt.subplots(\n",
    "#     div_factor, n_heads // div_factor, figsize=(10 * div_factor, 5 * div_factor)\n",
    "# )\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     weights = (\n",
    "#         aggregate_tensors(head_weights[i], mode=\"percentile\", percentile=99)\n",
    "#         .cpu()\n",
    "#         .float()\n",
    "#         .numpy()\n",
    "#     )\n",
    "#     sns.heatmap(\n",
    "#         weights,\n",
    "#         ax=ax,\n",
    "#         cmap=\"rocket\",\n",
    "#         yticklabels=False,\n",
    "#     )\n",
    "#     ax.set_title(f\"Head {i + 1}\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"imgs/99_percentile_per_head.png\")\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CONTEXT_LENGTH = 1024\n",
    "MAX_NEW_TOKENS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    return_dict_in_generate=True,\n",
    ").eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating qa1 split: 100 examples [00:00, 2677.38 examples/s]\n",
      "Generating qa2 split: 100 examples [00:00, 25055.58 examples/s]\n",
      "Generating qa3 split: 100 examples [00:00, 28554.05 examples/s]\n",
      "Generating qa4 split: 100 examples [00:00, 25030.16 examples/s]\n",
      "Generating qa5 split: 100 examples [00:00, 24558.25 examples/s]\n",
      "Generating qa6 split: 100 examples [00:00, 22762.97 examples/s]\n",
      "Generating qa7 split: 100 examples [00:00, 24107.97 examples/s]\n",
      "Generating qa8 split: 100 examples [00:00, 27784.21 examples/s]\n",
      "Generating qa9 split: 100 examples [00:00, 26588.30 examples/s]\n",
      "Generating qa10 split: 100 examples [00:00, 26886.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"RMT-team/babilong\", \"1k\", split=\"qa1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(story, question):\n",
    "    return f\"{story}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "def contains_needle(response, target):\n",
    "    return float(target.lower() in response.lower())\n",
    "\n",
    "def find_subtensor_indices(haystack: torch.LongTensor, needle: torch.LongTensor) -> torch.LongTensor:\n",
    "    n, m = haystack.size(0), needle.size(0)\n",
    "    if m == 0:\n",
    "        return torch.arange(n + 1, dtype=torch.long)\n",
    "    if m > n:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "    windows = haystack.unfold(0, m, 1)\n",
    "    matches = (windows == needle).all(dim=1)\n",
    "    return matches.nonzero(as_tuple=True)[0]\n",
    "\n",
    "def plot_attention(attentions, input_ids, needle_ids, layer=0):\n",
    "    indices = find_subtensor_indices(input_ids, needle_ids)\n",
    "    if len(indices) == 0:\n",
    "        print(\"Needle tokens not found in input. Skipping attention plot.\")\n",
    "        return\n",
    "    index_start = indices[0].item()\n",
    "    index_end = index_start + len(needle_ids)\n",
    "    attention = attentions[layer][0]  # shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 7, figsize=(28, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        sns.heatmap(\n",
    "            attention[i][index_start:index_end].detach().cpu().numpy(),\n",
    "            ax=ax, cmap=\"viridis\", yticklabels=False\n",
    "        )\n",
    "        ax.set_title(f\"Head {i + 1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Recall Score (contains_needle) over 5 samples: 0.400\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 5\n",
    "\n",
    "for i in range(total):\n",
    "    print(i)\n",
    "    sample = dataset[i]\n",
    "    prompt = format_prompt(sample[\"input\"], sample[\"question\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    generated_ids = output.sequences[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    if contains_needle(response, sample[\"target\"]):\n",
    "        correct += 1\n",
    "\n",
    "    # log first few\n",
    "    # if i < 2:\n",
    "    #     print(f\"===== SAMPLE {i + 1} =====\")\n",
    "    #     print(\"Target:\", sample[\"target\"])\n",
    "    #     print(\"Model Output:\", response)\n",
    "    #     print(\"Contains needle:\", contains_needle(response, sample[\"target\"]))\n",
    "    #     print()\n",
    "\n",
    "# Final score\n",
    "accuracy = correct / total\n",
    "print(f\"Recall Score (contains_needle) over {total} samples: {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
